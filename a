üõ†Ô∏è Step 1: Choose the Right AWS Glue Kernel
For processing a huge dataset (400M rows, 200 columns) in AWS Glue via JupyterLab 4, use the Glue PySpark kernel (gluepyspark).

Kernel	Use Case
gluepyspark	Best for running AWS Glue interactive sessions (Recommended ‚úÖ)
gluespark	Similar to gluepyspark, optimized for AWS Glue jobs
sparkmagic pyspark	Used for connecting to remote Spark clusters (not Glue)
sparkmagic spark	Generic Spark (not Glue-specific)

pip install aws-glue-sessions
!glue-session start


from awsglue.context import GlueContext
from pyspark.context import SparkContext
from awsglue.dynamicframe import DynamicFrame
from pyspark.sql import SparkSession
import boto3

# Initialize Spark and Glue Context
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session


# Read partitioned data from AWS Glue Data Catalog
dyf = glueContext.create_dynamic_frame.from_catalog(
    database="my_database",
    table_name="my_large_table",
    push_down_predicate="year='2025'"
)

# Convert to Spark DataFrame
df = dyf.toDF()
df.printSchema()
df.show(5)


# Filter and select important columns
df_filtered = df.filter(df["status"] == "active").select("id", "name", "email")

# Show sample data
df_filtered.show(10)


dyf_output = DynamicFrame.fromDF(df_filtered, glueContext)

glueContext.write_dynamic_frame.from_options(
    frame=dyf_output,
    connection_type="s3",
    connection_options={"path": "s3://my-output-bucket/processed-data/"},
    format="parquet"
)


push_down_predicate="year='2025' AND month='02'"


{
  "--enable-auto-scaling": "true",
  "--worker-type": "G.2X",
  "--number-of-workers": "10"
}


{
  "--job-bookmark-option": "job-bookmark-enable"
}

push_down_predicate="status='active'"
