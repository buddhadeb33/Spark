from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, avg, when

# Initialize Spark Session
spark = SparkSession.builder \
    .appName("BigDataETL") \
    .config("spark.sql.shuffle.partitions", "200") \
    .getOrCreate()

# Extract: Load Data from CSV
input_path = "s3://your-bucket/input-data.csv"  # Change to your source path
df = spark.read.option("header", "true").csv(input_path)

# Transform: Data Cleaning and Aggregation
cleaned_df = df.dropna()

# Example transformation: Convert column to correct data type
cleaned_df = cleaned_df.withColumn("age", col("age").cast("int"))

# Aggregation Example: Compute average age per category
grouped_df = cleaned_df.groupBy("category").agg(avg("age").alias("avg_age"))

# Load: Save to Parquet
output_path = "s3://your-bucket/output-data.parquet"  # Change to your destination path
grouped_df.write.mode("overwrite").parquet(output_path)

# Stop Spark Session
spark.stop()
ee
