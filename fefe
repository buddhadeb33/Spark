from pyspark.sql.functions import udf
from pyspark.sql.types import StringType

def slow_function(val):
    return val.lower()

slow_udf = udf(slow_function, StringType())
df = df.withColumn("new_col", slow_udf("old_col"))  # Very slow!


from pyspark.sql.window import Window
from pyspark.sql.functions import sum

window_spec = Window.partitionBy("category").orderBy("date")
df = df.withColumn("running_total", sum("sales").over(window_spec))
