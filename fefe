#!/usr/bin/env python
"""
Example AWS Glue Job Script
This script reads data from an input path, applies a simple transformation,
and writes the processed data to an output path.
"""

import sys
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.utils import getResolvedOptions
from awsglue.job import Job

# -----------------------------------------------------------------------------------
# Step 1. Parse arguments passed to the job.
# -----------------------------------------------------------------------------------
# Required arguments: JOB_NAME, INPUT_PATH, OUTPUT_PATH.
args = getResolvedOptions(sys.argv, ['JOB_NAME', 'INPUT_PATH', 'OUTPUT_PATH'])

# -----------------------------------------------------------------------------------
# Step 2. Initialize Spark and Glue contexts.
# -----------------------------------------------------------------------------------
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session

# -----------------------------------------------------------------------------------
# Step 3. Initialize the Glue Job.
# -----------------------------------------------------------------------------------
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# -----------------------------------------------------------------------------------
# Step 4. Read the input data.
# -----------------------------------------------------------------------------------
# Change "parquet" to your actual data format if needed (e.g., "csv", "json")
input_df = spark.read.format("parquet").load(args['INPUT_PATH'])
# Optionally, print the schema to verify the data structure
input_df.printSchema()

# -----------------------------------------------------------------------------------
# Step 5. Data Transformation Logic.
# -----------------------------------------------------------------------------------
# For example, filter out records where the 'id' column is NULL.
# Replace "id" with a column that exists in your dataset.
transformed_df = input_df.filter("id IS NOT NULL")

# You can perform additional transformations here...
# e.g., select columns, join with other data, aggregate, etc.

# -----------------------------------------------------------------------------------
# Step 6. Write the processed data to the output path.
# -----------------------------------------------------------------------------------
# This example writes the data in Parquet format. Modify as needed.
transformed_df.write.mode("overwrite").parquet(args['OUTPUT_PATH'])

# -----------------------------------------------------------------------------------
# Step 7. Commit the job.
# -----------------------------------------------------------------------------------
job.commit()
